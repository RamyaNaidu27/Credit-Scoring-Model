{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356cec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1076c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\gowth\\anaconda3\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf71898",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_excel('a_Dataset_CreditScoring.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23eac60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1d850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET               0\n",
       "ID                   0\n",
       "DerogCnt             0\n",
       "CollectCnt           0\n",
       "BanruptcyInd         0\n",
       "InqCnt06             0\n",
       "InqTimeLast        188\n",
       "InqFinanceCnt24      0\n",
       "TLTimeFirst          0\n",
       "TLTimeLast           0\n",
       "TLCnt03              0\n",
       "TLCnt12              0\n",
       "TLCnt24              0\n",
       "TLCnt                3\n",
       "TLSum               40\n",
       "TLMaxSum            40\n",
       "TLSatCnt             4\n",
       "TLDel60Cnt           0\n",
       "TLBadCnt24           0\n",
       "TL75UtilCnt         99\n",
       "TL50UtilCnt         99\n",
       "TLBalHCPct          41\n",
       "TLSatPct             4\n",
       "TLDel3060Cnt24       0\n",
       "TLDel90Cnt24         0\n",
       "TLDel60CntAll        0\n",
       "TLOpenPct            3\n",
       "TLBadDerogCnt        0\n",
       "TLDel60Cnt24         0\n",
       "TLOpen24Pct          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924a1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.fillna(dataset.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55991f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET             0\n",
       "ID                 0\n",
       "DerogCnt           0\n",
       "CollectCnt         0\n",
       "BanruptcyInd       0\n",
       "InqCnt06           0\n",
       "InqTimeLast        0\n",
       "InqFinanceCnt24    0\n",
       "TLTimeFirst        0\n",
       "TLTimeLast         0\n",
       "TLCnt03            0\n",
       "TLCnt12            0\n",
       "TLCnt24            0\n",
       "TLCnt              0\n",
       "TLSum              0\n",
       "TLMaxSum           0\n",
       "TLSatCnt           0\n",
       "TLDel60Cnt         0\n",
       "TLBadCnt24         0\n",
       "TL75UtilCnt        0\n",
       "TL50UtilCnt        0\n",
       "TLBalHCPct         0\n",
       "TLSatPct           0\n",
       "TLDel3060Cnt24     0\n",
       "TLDel90Cnt24       0\n",
       "TLDel60CntAll      0\n",
       "TLOpenPct          0\n",
       "TLBadDerogCnt      0\n",
       "TLDel60Cnt24       0\n",
       "TLOpen24Pct        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9fbb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dataset.iloc[:,0].values\n",
    "x=dataset.iloc[:,1:29].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212a49f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "(3000, 28)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "254ca6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25,random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96c8c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977a11ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\gowth\\\\OneDrive\\\\Desktop\\\\CODE-ALPHA\\\\futureuse_normalisation_creditscoring.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(sc, r'C:\\Users\\gowth\\OneDrive\\Desktop\\CODE-ALPHA\\futureuse_normalisation_creditscoring.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ff6b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =  LogisticRegression()#This initialises an object 'classifier' from the sklearn library or the sklearn.linear_model , to be more specific\n",
    "classifier.fit(x_train, y_train)#This is to train the classifier object on  training data  ,as,x_train contains the feature data (independent variables) and y_train contains the corresponding labels/targets (dependent variable,here 0 or 1).\n",
    "#The fit method adjusts weights of the model and minimises the error in predicting y_train from x_train.It is a commonly used method\n",
    "y_pred = classifier.predict(x_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07fc48a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\gowth\\\\OneDrive\\\\Desktop\\\\CODE-ALPHA\\\\futureuse_Classifier_CreditScoring.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(classifier, r'C:\\Users\\gowth\\OneDrive\\Desktop\\CODE-ALPHA\\futureuse_Classifier_CreditScoring.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a66bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[609  16]\n",
      " [105  20]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c07a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fractional accuracy is:0.8386666666666667\n",
      "The accuracy percentage is:83.86666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"The fractional accuracy is:{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"The accuracy percentage is:{accuracy_score(y_test, y_pred)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "456ea237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93489876, 0.06510124],\n",
       "       [0.90853572, 0.09146428],\n",
       "       [0.90968011, 0.09031989],\n",
       "       ...,\n",
       "       [0.87596545, 0.12403455],\n",
       "       [0.87717214, 0.12282786],\n",
       "       [0.53797816, 0.46202184]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = classifier.predict_proba(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "451d8f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Outcome</th>\n",
       "      <th>Probability 0</th>\n",
       "      <th>Probability 1</th>\n",
       "      <th>Predicted Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.934899</td>\n",
       "      <td>0.065101</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908536</td>\n",
       "      <td>0.091464</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.909680</td>\n",
       "      <td>0.090320</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834471</td>\n",
       "      <td>0.165529</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813557</td>\n",
       "      <td>0.186443</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual Outcome  Probability 0  Probability 1  Predicted Target\n",
       "0             1.0       0.934899       0.065101               0.0\n",
       "1             0.0       0.908536       0.091464               0.0\n",
       "2             0.0       0.909680       0.090320               0.0\n",
       "3             0.0       0.834471       0.165529               0.0\n",
       "4             0.0       0.813557       0.186443               0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing model output file\n",
    "\n",
    "df_pred_prob = pd.DataFrame(predictions, columns = ['Probability 0', 'Probability 1'])#This line creates a DataFrame from the 'predictions' array in the above cell, which contains the probabilities of each test sample belonging to class/category 0 and class/category 1. Columns are named 'Probability 0' and 'Probability 1'.\n",
    "\n",
    "df_pred_target = pd.DataFrame(classifier.predict(x_test), columns = ['Predicted Target'])#This creates another DataFrame from the 'predictions' made by the classifier (object) (i.e. class labels, not the probabilities), with a single column as 'Predicted Target'.\n",
    "\n",
    "df_test_dataset = pd.DataFrame(y_test,columns= ['Actual Outcome'])#This DataFrame is made from 'y_test, which contains the actual class 'labels' or 'mappings' for the test data, named 'Actual Outcome'.\n",
    "\n",
    "\n",
    "df_x=pd.concat([df_test_dataset, df_pred_prob, df_pred_target], axis=1)#This specific line concatenates the three DataFrames along the columns (axis=1), resulting in a single DataFrame 'df_x' that includes the actual outcomes, the predicted probabilities, and the predicted class labels for each test datapoint/sample.\n",
    "\n",
    "df_x.to_csv(r\"C:\\Users\\gowth\\OneDrive\\Desktop\\CODE-ALPHA\\Model_Prediction.csv\", sep=',', encoding='UTF-8')\n",
    "#This saves the DataFrame 'df_x' to a CSV file. The specific path and file name indicate it is saved as an 'Excel' file, but since the method 'to_csv' has been used,the file will actually be in CSV format, not XLSX.\n",
    "#So can also save it as '.csv' for more clarity.\n",
    "df_x.head()#To print the first few rows(5 specifically) of the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eda5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
